{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, TypedDict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import logging\n",
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "from scipy.stats import dirichlet\n",
    "from openai import OpenAI  # Ensure OpenAI library is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for the CERT dataset\n",
    "logon_file = \"r4.1/logon.csv\"\n",
    "file_access_file = \"r4.1/file.csv\"\n",
    "email_file = \"r4.1/email.csv\"\n",
    "device_file = \"r4.1/device.csv\"\n",
    "http_file = \"r4.1/http.csv\"\n",
    "psychometric_file = \"r4.1/psychometric.csv\"\n",
    "ground_truth_file = \"answers/r4.1-1.csv\"\n",
    "\n",
    "# Load dataset\n",
    "logon_df = pd.read_csv(logon_file)\n",
    "file_access_df = pd.read_csv(file_access_file)\n",
    "email_df = pd.read_csv(email_file)\n",
    "device_df = pd.read_csv(device_file)\n",
    "http_df = pd.read_csv(http_file)\n",
    "psychometric_df = pd.read_csv(psychometric_file)\n",
    "ground_truth_df = pd.read_csv(ground_truth_file, names=[\"log_type\", \"id\", \"date\", \"user\", \"pc\", \"activity_or_url\", \"content\"], header=None)\n",
    "\n",
    "# Define the CERT data dictionary\n",
    "data_dict = {\n",
    "    'logon': logon_df,\n",
    "    'file_access': file_access_df,\n",
    "    'email': email_df,\n",
    "    'device': device_df,\n",
    "    'http': http_df\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: logon, Shape: (899118, 5)\n",
      "                         id                 date     user       pc activity\n",
      "0  {X0W9-Q2DW16EI-1074QDVQ}  01/02/2010 05:02:50  WCR0044  PC-9174    Logon\n",
      "1  {C2O4-Z2RH12FQ-9176MUEL}  01/02/2010 05:19:09  WCR0044  PC-9174   Logoff\n",
      "2  {U1J8-P4HX02EV-5327GONH}  01/02/2010 06:22:11  WCR0044  PC-5494    Logon\n",
      "3  {F1N9-G4ZL24LA-8747VGHG}  01/02/2010 06:33:00  LRG0155  PC-0450    Logon\n",
      "4  {Y1Q0-U9BN24NB-1906LMVT}  01/02/2010 06:42:00  RHM0148  PC-8152    Logon\n",
      "Dataset: file_access, Shape: (414556, 6)\n",
      "                         id                 date     user       pc  \\\n",
      "0  {Q5M2-N0JR22AA-1727ALDU}  01/02/2010 05:15:35  WCR0044  PC-9174   \n",
      "1  {O2W1-I9JA58XQ-7973YYAK}  01/02/2010 05:16:04  WCR0044  PC-9174   \n",
      "2  {O0T0-H0OD25UT-0238OGNR}  01/02/2010 05:16:25  WCR0044  PC-9174   \n",
      "3  {Y5P3-S3DI98OH-7978PRAW}  01/02/2010 05:16:28  WCR0044  PC-9174   \n",
      "4  {C5T9-R4OO02RR-0266RBKQ}  01/02/2010 05:16:29  WCR0044  PC-9174   \n",
      "\n",
      "       filename                                            content  \n",
      "0  KQTA0DRL.pdf  25-50-44-46-2D laugh statements references pla...  \n",
      "1  PS5NPZPV.doc  D0-CF-11-E0-A1-B1-1A-E1 identifies new maxi do...  \n",
      "2  VJX6WCC9.doc  D0-CF-11-E0-A1-B1-1A-E1 emotionally antagonist...  \n",
      "3  726ERRX7.doc  D0-CF-11-E0-A1-B1-1A-E1 park central personal ...  \n",
      "4  38FJLPW2.jpg                                              FF-D8  \n",
      "Dataset: email, Shape: (2733360, 11)\n",
      "                         id                 date     user       pc  \\\n",
      "0  {C0K9-N2TG12WK-3772OEPN}  01/02/2010 06:50:45  LRG0155  PC-0450   \n",
      "1  {F5Y9-A8ON18GE-5541BCZB}  01/02/2010 07:05:31  KAB0942  PC-8686   \n",
      "2  {I8R9-T8NW12WZ-6603QAOH}  01/02/2010 07:06:46  KAB0942  PC-8686   \n",
      "3  {K1Z8-V3QS79OJ-1650RODD}  01/02/2010 07:16:05  ECM0654  PC-7356   \n",
      "4  {E2B2-Z6AA12HB-1496LJRN}  01/02/2010 07:16:29  ECM0654  PC-7356   \n",
      "\n",
      "                                to                            cc  \\\n",
      "0      Rafael.H.Mccall@netzero.com                           NaN   \n",
      "1     Ali.Cole.Mclaughlin@dtaa.com  Karly.Amity.Bentley@dtaa.com   \n",
      "2  Brent.Colorado.Sanders@dtaa.com                           NaN   \n",
      "3        Lane.L.Clay@optonline.net                           NaN   \n",
      "4        Abigail_Tyson@comcast.net      Martin-Erica@netzero.com   \n",
      "\n",
      "                             bcc                           from   size  \\\n",
      "0  Lysandra_Guerrero@netzero.com  Lysandra_Guerrero@netzero.com  29918   \n",
      "1                            NaN   Karly.Amity.Bentley@dtaa.com  22235   \n",
      "2   Karly.Amity.Bentley@dtaa.com   Karly.Amity.Bentley@dtaa.com  54903   \n",
      "3       Martin-Erica@netzero.com       Martin-Erica@netzero.com  21294   \n",
      "4                            NaN       Martin-Erica@netzero.com  25360   \n",
      "\n",
      "   attachments                                            content  \n",
      "0            0  apple lot mundane response dark who is ralbovs...  \n",
      "1            0  again 15 bought always avoided reaching joined...  \n",
      "2            0  evers guns what decided baltimore forced docto...  \n",
      "3            0  method knew million took freeeasy four left ro...  \n",
      "4            0  new india established size inflicting frontier...  \n",
      "Dataset: device, Shape: (437168, 5)\n",
      "                         id                 date     user       pc    activity\n",
      "0  {P1I9-X2UT34LR-6485PDJX}  01/02/2010 05:15:32  WCR0044  PC-9174     Connect\n",
      "1  {S2D2-V1NT65KD-8108KBSZ}  01/02/2010 05:17:09  WCR0044  PC-9174  Disconnect\n",
      "2  {G5Y9-L4GP40WG-0372GEOX}  01/02/2010 06:39:10  WCR0044  PC-5494     Connect\n",
      "3  {A4Q5-A1FR45TY-9562ISJI}  01/02/2010 06:40:30  WCR0044  PC-5494  Disconnect\n",
      "4  {M9Q0-O7UM21OB-5937NRWH}  01/02/2010 07:02:33  BHV0556  PC-6254     Connect\n",
      "Dataset: http, Shape: (29553383, 6)\n",
      "                         id                 date     user       pc  \\\n",
      "0  {W1D0-Y6WN34LF-1843MHXW}  01/02/2010 06:35:31  LRG0155  PC-0450   \n",
      "1  {H6X2-S7KB89GP-5832OBFJ}  01/02/2010 06:38:35  LRG0155  PC-0450   \n",
      "2  {K1U0-I6CV48OO-1582ZFRV}  01/02/2010 06:41:14  LRG0155  PC-0450   \n",
      "3  {T5T8-R9CU98WX-3061CALI}  01/02/2010 06:47:29  BHV0556  PC-6254   \n",
      "4  {S5B6-V3DR67ST-6152TMBT}  01/02/2010 06:47:58  BHV0556  PC-6254   \n",
      "\n",
      "                                                 url  \\\n",
      "0  http://groupon.com/1980_eruption_of_Mount_St_H...   \n",
      "1  http://opendns.com/British_Empire/rowlatt/uvxv...   \n",
      "2  http://sidereel.com/Miniopterus_griveaudi/griv...   \n",
      "3  http://td.com/History_of_the_National_Hockey_L...   \n",
      "4  http://buzzfeed.com/William_III_of_England/sta...   \n",
      "\n",
      "                                             content  \n",
      "0  alone on they t1517 residual m7 73 sample subs...  \n",
      "1  england river refused soon shared relationship...  \n",
      "2  lane ready late toss you claimed too jumped 36...  \n",
      "3  part people orphaned northeast charles hour te...  \n",
      "4  almost explain radar least european event more...  \n"
     ]
    }
   ],
   "source": [
    "# Print the shape and first few rows of each DataFrame to verify\n",
    "for key, df in data_dict.items():\n",
    "    print(f\"Dataset: {key}, Shape: {df.shape}\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UncertaintyAwareRLToolBuilderAgent:\n",
    "    def __init__(self, api_key, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1, max_retries=10, num_actions=3):\n",
    "        self.api_key = api_key\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.tools = {}\n",
    "        self.q_table = defaultdict(lambda: defaultdict(lambda: 1e-3))  # Initialize Q-values to a small positive value\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.max_retries = max_retries\n",
    "        self.num_actions = num_actions  # Number of available actions\n",
    "        self.uncertainties = {\"vacuity\": [], \"dissonance\": [], \"entropy\": []}\n",
    "\n",
    "    def choose_action(self, state: str) -> int:\n",
    "        \"\"\"Choose an action based on the state and Q-table using Dirichlet probabilities.\"\"\"\n",
    "        action_probabilities = self.get_action_probabilities(state)\n",
    "        \n",
    "        # Handle invalid probabilities\n",
    "        if np.any(np.isnan(action_probabilities)):\n",
    "            logging.error(f\"Action probabilities contain NaN for state '{state}'. Defaulting to uniform probabilities.\")\n",
    "            action_probabilities = np.full(self.num_actions, 1 / self.num_actions)\n",
    "        \n",
    "        action = np.random.choice(range(self.num_actions), p=action_probabilities)\n",
    "        logging.info(f\"Chosen action with probabilities {action_probabilities}: {action}\")\n",
    "        return action\n",
    "\n",
    "    def get_action_probabilities(self, state: str) -> List[float]:\n",
    "        \"\"\"Compute action probabilities using Dirichlet distribution.\"\"\"\n",
    "        action_counts = [self.q_table[state].get(a, 0) + 1e-3 for a in range(self.num_actions)]\n",
    "        \n",
    "        # Ensure all parameters are positive for the Dirichlet distribution\n",
    "        if any(count <= 0 for count in action_counts):\n",
    "            logging.warning(f\"Invalid Dirichlet parameters for state '{state}': {action_counts}. Fixing to minimum value.\")\n",
    "            action_counts = [max(count, 1e-3) for count in action_counts]\n",
    "        \n",
    "        probabilities = dirichlet.rvs(action_counts, size=1).flatten()\n",
    "        \n",
    "        # Normalize probabilities to ensure they sum to 1\n",
    "        probabilities = probabilities / probabilities.sum() if probabilities.sum() > 0 else np.full(self.num_actions, 1 / self.num_actions)\n",
    "        \n",
    "        return probabilities\n",
    "\n",
    "\n",
    "    def execute_with_retry(self, func, df, max_retries=3, **kwargs):\n",
    "        \"\"\"\n",
    "        Executes a function with retry mechanism. Fixes known issues like index mismatch.\n",
    "        \"\"\"\n",
    "        attempt = 0\n",
    "        while attempt < max_retries:\n",
    "            try:\n",
    "                return func(df, **kwargs)\n",
    "            except ValueError as ve:\n",
    "                logging.error(f\"Execution attempt {attempt + 1} failed: {ve}\")\n",
    "                if \"incompatible index of inserted column\" in str(ve):\n",
    "                    df = df.reset_index(drop=True)  # Reset index to fix the mismatch\n",
    "                    logging.info(\"Index mismatch detected. Resetting dataframe index and retrying.\")\n",
    "                else:\n",
    "                    logging.error(f\"Unhandled ValueError: {ve}\")\n",
    "                    raise ve\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Execution attempt {attempt + 1} failed: {e}\")\n",
    "            attempt += 1\n",
    "        logging.error(f\"Function execution failed after {max_retries} attempts.\")\n",
    "        return None\n",
    "\n",
    "    def update_uncertainty_measures(self, action_probabilities: List[float]):\n",
    "        \"\"\"Update vacuity, dissonance, and entropy measures.\"\"\"\n",
    "        vacuity = 1 - sum(action_probabilities) / self.num_actions\n",
    "        dissonance = sum(\n",
    "            abs(a_i - a_j)\n",
    "            for i, a_i in enumerate(action_probabilities)\n",
    "            for j, a_j in enumerate(action_probabilities)\n",
    "            if i != j\n",
    "        ) / (2 * self.num_actions)\n",
    "        entropy = -sum(p * np.log2(p) if p > 0 else 0 for p in action_probabilities) / np.log2(self.num_actions)\n",
    "\n",
    "        self.uncertainties[\"vacuity\"].append(vacuity)\n",
    "        self.uncertainties[\"dissonance\"].append(dissonance)\n",
    "        self.uncertainties[\"entropy\"].append(entropy)\n",
    "\n",
    "        logging.info(f\"Updated uncertainty measures: Vacuity={vacuity}, Dissonance={dissonance}, Entropy={entropy}\")\n",
    "\n",
    "    def generate_tool_code(self, task_name: str, requirements: str, prompt_style: str, columns: List[str], custom_prompt: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate code for a specific tool using GPT.\n",
    "        Accepts a custom prompt if provided.\n",
    "        \"\"\"\n",
    "        prompt = custom_prompt or f\"\"\"\n",
    "        Generate a complete, well-formatted Python function named '{task_name}' to detect {requirements}.\n",
    "        Ensure column names ({', '.join(columns)}) are validated, with detailed error handling.\n",
    "        \"\"\"\n",
    "\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=\"gpt-4\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=2000\n",
    "                )\n",
    "                raw_text = response.choices[0].message.content.strip()\n",
    "                code = self.extract_code(raw_text)\n",
    "                if code and self.is_code_valid(code):\n",
    "                    logging.info(f\"Generated code after {attempt + 1} attempt(s)\")\n",
    "                    return code\n",
    "            except Exception as e:\n",
    "                logging.error(f\"OpenAI API error: {e}\")\n",
    "                continue\n",
    "\n",
    "            logging.warning(f\"Retry {attempt + 1}/{self.max_retries}: Syntax issue, retrying...\")\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_code(response_text: str) -> str:\n",
    "        \"\"\"Extract Python code block from GPT response.\"\"\"\n",
    "        code_match = re.search(r\"```python\\n(.*?)\\n```\", response_text, re.DOTALL)\n",
    "        return code_match.group(1) if code_match else response_text\n",
    "\n",
    "    @staticmethod\n",
    "    def is_code_valid(code: str) -> bool:\n",
    "        \"\"\"Check if the generated code is syntactically valid.\"\"\"\n",
    "        try:\n",
    "            compile(code, \"<string>\", \"exec\")\n",
    "            return True\n",
    "        except SyntaxError as e:\n",
    "            logging.error(f\"Syntax issue: {e}\")\n",
    "            return False\n",
    "\n",
    "    def evaluate_tool(self, tool_name: str):\n",
    "        tool_func = self.tools.get(tool_name)\n",
    "        if tool_func:\n",
    "            test_data = self._get_test_data(tool_name)  # Use the defined method\n",
    "            try:\n",
    "                return self.execute_with_retry(tool_func, test_data)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during evaluation: {e}\")\n",
    "                return -5\n",
    "        logging.error(f\"Tool {tool_name} not found.\")\n",
    "        return -10\n",
    "\n",
    "    def create_tool(self, task_name: str, requirements: str, columns: List[str]):\n",
    "        \"\"\"Main function to create and evaluate a tool.\"\"\"\n",
    "        state = task_name\n",
    "        action = self.choose_action(state)\n",
    "\n",
    "        tool_code = self.generate_tool_code(task_name, requirements, \"default_prompt\", columns)\n",
    "        if tool_code is None:\n",
    "            logging.error(f\"Code generation failed for {task_name} after maximum retries.\")\n",
    "            self.update_q_table(state, action, -10)\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Debug: Print the generated code\n",
    "            print(\"Generated Code:\")\n",
    "            print(tool_code)\n",
    "            \n",
    "            exec(tool_code, globals())\n",
    "            func_name = task_name.replace(' ', '_')\n",
    "            # Debug: Check if the function exists in globals\n",
    "            print(f\"Looking for function: {func_name} in globals.\")\n",
    "            self.tools[task_name] = globals().get(func_name)\n",
    "            \n",
    "            if self.tools[task_name] is None:\n",
    "                logging.error(f\"Function {func_name} was not created successfully.\")\n",
    "                self.update_q_table(state, action, -10)\n",
    "                return\n",
    "\n",
    "            reward = self.evaluate_tool(func_name)\n",
    "            self.update_q_table(state, action, reward)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Tool generation failed for {task_name}: {e}\")\n",
    "        self.update_q_table(state, action, -10)\n",
    "\n",
    "    def update_q_table(self, state: str, action: int, reward: float):\n",
    "        \"\"\"Update Q-table based on the RL update rule, ensuring non-negative Q-values.\"\"\"\n",
    "        current_q = self.q_table[state][action]\n",
    "        best_future_q = max(self.q_table[state].values(), default=0)\n",
    "        new_q = max(\n",
    "            current_q + self.learning_rate * (reward + self.discount_factor * best_future_q - current_q),\n",
    "            1e-3  # Ensure Q-values are at least 1e-3\n",
    "        )\n",
    "        self.q_table[state][action] = new_q\n",
    "        logging.info(f\"Q-table updated: State={state}, Action={action}, Q-value={new_q}\")\n",
    "\n",
    "    def _get_test_data(self, tool_name: str):\n",
    "        # Example test data based on expected input\n",
    "        return pd.DataFrame({\n",
    "            \"user\": [\"WCR0044\", \"LRG0155\"],\n",
    "            \"date\": [\"2024-01-02 05:02:50\", \"2024-01-02 06:33:00\"],\n",
    "            \"pc\": [\"PC-9174\", \"PC-0450\"],\n",
    "            \"activity\": [\"Logon\", \"Logoff\"],\n",
    "            \"content\": [\"Login successful\", \"User logged out\"]\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Assume `UncertaintyAwareRLToolBuilderAgent` is already defined and tested.\n",
    "\n",
    "# Decomposer Agent\n",
    "class DecomposerAgent:\n",
    "    def __init__(self, task_name: str):\n",
    "        self.task_name = task_name\n",
    "\n",
    "    def decompose_task(self) -> Dict[str, str]:\n",
    "        log_types = ['logon', 'psychometric', 'file_access', 'email', 'device']\n",
    "        subtasks = {log_type: f\"{self.task_name}_{log_type}\" for log_type in log_types}\n",
    "        logging.info(f\"Decomposed task '{self.task_name}' into subtasks: {list(subtasks.values())}\")\n",
    "        return subtasks\n",
    "\n",
    "\n",
    "# Executor Node\n",
    "class ExecutorNode:\n",
    "    def __init__(self, tool_name: str, tool_func):\n",
    "        self.tool_name = tool_name\n",
    "        self.tool_func = tool_func\n",
    "\n",
    "    def execute(self, df: pd.DataFrame, *args, **kwargs):\n",
    "        if not callable(self.tool_func):\n",
    "            logging.error(f\"Tool {self.tool_name} is not callable.\")\n",
    "            return None\n",
    "        try:\n",
    "            return self.tool_func(df, *args, **kwargs)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error executing tool {self.tool_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# Anomaly Aggregator Agent\n",
    "class AnomalyAggregatorAgent:\n",
    "    def aggregate(self, results: Dict[str, pd.DataFrame]):\n",
    "        aggregated_results = {}\n",
    "        for log_type, result in results.items():\n",
    "            if isinstance(result, pd.DataFrame):\n",
    "                if result.empty:\n",
    "                    aggregated_results[log_type] = {\n",
    "                        \"anomalies\": None,\n",
    "                        \"reasons\": f\"No anomalies detected (empty DataFrame) for {log_type}\"\n",
    "                    }\n",
    "                else:\n",
    "                    aggregated_results[log_type] = {\n",
    "                        \"anomalies\": result,\n",
    "                        \"reasons\": f\"Flagged {len(result)} suspicious entries for {log_type}\"\n",
    "                    }\n",
    "            elif result is None:\n",
    "                aggregated_results[log_type] = {\n",
    "                    \"anomalies\": None,\n",
    "                    \"reasons\": f\"No anomalies detected or tool failed for {log_type}\"\n",
    "                }\n",
    "            else:\n",
    "                logging.warning(f\"Unexpected result type for {log_type}: {type(result).__name__}\")\n",
    "                aggregated_results[log_type] = {\n",
    "                    \"anomalies\": None,\n",
    "                    \"reasons\": f\"Tool returned an invalid result for {log_type}\"\n",
    "                }\n",
    "        logging.info(f\"Final Aggregated Anomalies:\\n{aggregated_results}\")\n",
    "        return aggregated_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize agents\n",
    "task_name = \"Detect_Suspicious_Activity\"\n",
    "decomposer_agent = DecomposerAgent(task_name)\n",
    "anomaly_aggregator_agent = AnomalyAggregatorAgent()\n",
    "rl_tool_builder_agent = UncertaintyAwareRLToolBuilderAgent(\n",
    "    api_key= OPENAI_API_KEY,\n",
    "    learning_rate=0.1,\n",
    "    discount_factor=0.9,\n",
    "    exploration_rate=0.1,\n",
    "    max_retries=10,\n",
    "    num_actions=3\n",
    ")\n",
    "\n",
    "data_sources = {\n",
    "    \"logon\": pd.DataFrame([\n",
    "        [\"WCR0044\", \"2024-01-02 05:02:50\", \"PC-9174\", \"Logon\", \"Login successful\"],\n",
    "        [\"WCR0044\", \"2024-01-02 06:15:30\", \"PC-9174\", \"Logon\", \"Login successful\"],\n",
    "        [\"LRG0155\", \"2024-01-02 06:33:00\", \"PC-0450\", \"Logoff\", \"User logged out\"],\n",
    "        [\"XTR0011\", \"2024-01-02 07:45:00\", \"PC-1234\", \"Logon\", \"Login successful\"],\n",
    "        [\"FLX0909\", \"2024-01-02 08:15:00\", \"PC-9987\", \"Logon\", \"Multiple failed login attempts\"],\n",
    "    ], columns=[\"user\", \"date\", \"pc\", \"activity\", \"content\"]),\n",
    "\n",
    "    \"psychometric\": pd.DataFrame([\n",
    "        {\"user\": \"WCR0044\", \"trait\": \"openness\", \"value\": 0.8},\n",
    "        {\"user\": \"LRG0155\", \"trait\": \"conscientiousness\", \"value\": 0.6},\n",
    "        {\"user\": \"FLX0909\", \"trait\": \"neuroticism\", \"value\": 0.9},\n",
    "    ]),\n",
    "\n",
    "    \"file_access\": pd.DataFrame([\n",
    "        {\"user\": \"WCR0044\", \"file\": \"confidential.pdf\", \"access_type\": \"read\", \"timestamp\": \"2024-01-02 05:15:00\"},\n",
    "        {\"user\": \"LRG0155\", \"file\": \"project_plan.docx\", \"access_type\": \"write\", \"timestamp\": \"2024-01-02 06:40:00\"},\n",
    "        {\"user\": \"FLX0909\", \"file\": \"budget.xlsx\", \"access_type\": \"delete\", \"timestamp\": \"2024-01-02 07:30:00\"},\n",
    "    ]),\n",
    "\n",
    "    \"email\": pd.DataFrame([\n",
    "        {\"user\": \"LRG0155\", \"email_content\": \"Suspicious email content\", \"timestamp\": \"2024-01-02 06:50:00\"},\n",
    "        {\"user\": \"FLX0909\", \"email_content\": \"Confidential project details\", \"timestamp\": \"2024-01-02 07:15:00\"},\n",
    "        {\"user\": \"XTR0011\", \"email_content\": \"Unauthorized access attempt\", \"timestamp\": \"2024-01-02 08:00:00\"},\n",
    "    ]),\n",
    "\n",
    "    \"device\": pd.DataFrame([\n",
    "        {\"user\": \"XTR0011\", \"device\": \"USB\", \"action\": \"connect\", \"timestamp\": \"2024-01-02 07:00:00\"},\n",
    "        {\"user\": \"FLX0909\", \"device\": \"External HDD\", \"action\": \"disconnect\", \"timestamp\": \"2024-01-02 08:30:00\"},\n",
    "        {\"user\": \"WCR0044\", \"device\": \"Printer\", \"action\": \"connect\", \"timestamp\": \"2024-01-02 09:00:00\"},\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Standardize column names to lowercase\n",
    "for log_type, df in data_sources.items():\n",
    "    data_sources[log_type].columns = df.columns.str.lower()\n",
    "\n",
    "# Convert relevant columns to datetime format\n",
    "datetime_columns = {\n",
    "    \"logon\": [\"date\"],\n",
    "    \"file_access\": [\"timestamp\"],\n",
    "    \"email\": [\"timestamp\"],\n",
    "    \"device\": [\"timestamp\"]\n",
    "}\n",
    "for log_type, cols in datetime_columns.items():\n",
    "    for col in cols:\n",
    "        if col in data_sources[log_type].columns:\n",
    "            data_sources[log_type][col] = pd.to_datetime(data_sources[log_type][col], errors='coerce')\n",
    "\n",
    "\n",
    "    \n",
    "def safe_execute_tool(tool_func, df: pd.DataFrame, **kwargs):\n",
    "    \"\"\"\n",
    "    Safely execute the tool function and ensure it returns a DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = tool_func(df, **kwargs)\n",
    "        if not isinstance(result, pd.DataFrame):  # Ensure the result is a DataFrame\n",
    "            raise ValueError(f\"Tool did not return a valid DataFrame. Got {type(result).__name__} instead.\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unhandled exception during tool execution for {tool_func.__name__}: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame on error\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Decomposed task 'Detect_Suspicious_Activity' into subtasks: ['Detect_Suspicious_Activity_logon', 'Detect_Suspicious_Activity_psychometric', 'Detect_Suspicious_Activity_file_access', 'Detect_Suspicious_Activity_email', 'Detect_Suspicious_Activity_device']\n",
      "INFO:root:Creating tool for subtask: Detect_Suspicious_Activity_logon\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Generated code after 1 attempt(s)\n",
      "INFO:root:Creating tool for subtask: Detect_Suspicious_Activity_psychometric\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Generated code after 1 attempt(s)\n",
      "INFO:root:Creating tool for subtask: Detect_Suspicious_Activity_file_access\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user              trait  value  is_suspicious\n",
      "1  LRG0155  conscientiousness    1.2           True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Generated code after 1 attempt(s)\n",
      "INFO:root:Creating tool for subtask: Detect_Suspicious_Activity_email\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Generated code after 1 attempt(s)\n",
      "INFO:root:Creating tool for subtask: Detect_Suspicious_Activity_device\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Generated code after 1 attempt(s)\n",
      "INFO:root:Final Aggregated Anomalies:\n",
      "{'logon': {'anomalies':       user                date       pc activity           content\n",
      "0  WCR0044 2024-01-02 05:02:50  PC-9174    Logon  Login successful\n",
      "1  WCR0044 2024-01-02 06:15:30  PC-9174    Logon  Login successful\n",
      "2  LRG0155 2024-01-02 06:33:00  PC-0450   Logoff   User logged out\n",
      "3  XTR0011 2024-01-02 07:45:00  PC-1234    Logon  Login successful, 'reasons': 'Flagged 4 suspicious entries for logon'}, 'psychometric': {'anomalies': None, 'reasons': 'No anomalies detected (empty DataFrame) for psychometric'}, 'file_access': {'anomalies':       user         file access_type           timestamp\n",
      "2  FLX0909  budget.xlsx      delete 2024-01-02 07:30:00, 'reasons': 'Flagged 1 suspicious entries for file_access'}, 'email': {'anomalies': None, 'reasons': 'No anomalies detected (empty DataFrame) for email'}, 'device': {'anomalies':       user device   action           timestamp\n",
      "0  XTR0011    USB  connect 2024-01-02 07:00:00, 'reasons': 'Flagged 1 suspicious entries for device'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Aggregated Anomalies:\n",
      "{'logon': {'anomalies':       user                date       pc activity           content\n",
      "0  WCR0044 2024-01-02 05:02:50  PC-9174    Logon  Login successful\n",
      "1  WCR0044 2024-01-02 06:15:30  PC-9174    Logon  Login successful\n",
      "2  LRG0155 2024-01-02 06:33:00  PC-0450   Logoff   User logged out\n",
      "3  XTR0011 2024-01-02 07:45:00  PC-1234    Logon  Login successful, 'reasons': 'Flagged 4 suspicious entries for logon'}, 'psychometric': {'anomalies': None, 'reasons': 'No anomalies detected (empty DataFrame) for psychometric'}, 'file_access': {'anomalies':       user         file access_type           timestamp\n",
      "2  FLX0909  budget.xlsx      delete 2024-01-02 07:30:00, 'reasons': 'Flagged 1 suspicious entries for file_access'}, 'email': {'anomalies': None, 'reasons': 'No anomalies detected (empty DataFrame) for email'}, 'device': {'anomalies':       user device   action           timestamp\n",
      "0  XTR0011    USB  connect 2024-01-02 07:00:00, 'reasons': 'Flagged 1 suspicious entries for device'}}\n"
     ]
    }
   ],
   "source": [
    "# Updated prompt and schema specification in the pipeline\n",
    "import logging\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Define Input Schema and Objectives for Each Subtask\n",
    "tool_specs = {\n",
    "    \"logon\": {\n",
    "        \"required_columns\": [\"user\", \"date\", \"pc\", \"activity\", \"content\"],\n",
    "        \"objective\": \"Identify suspicious logon activity outside working hours (8 AM to 6 PM) or invalid activity types.\",\n",
    "        \"example_input\": {\n",
    "            \"user\": [\"WCR0044\", \"LRG0155\"],\n",
    "            \"date\": [\"2024-01-02 05:02:50\", \"2024-01-02 06:33:00\"],\n",
    "            \"pc\": [\"PC-9174\", \"PC-0450\"],\n",
    "            \"activity\": [\"Logon\", \"Logoff\"],\n",
    "            \"content\": [\"Login successful\", \"User logged out\"]\n",
    "        },\n",
    "        \"example_output\": \"Filtered DataFrame containing rows with suspicious logon activities.\"\n",
    "    },\n",
    "    \"file_access\": {\n",
    "        \"required_columns\": [\"user\", \"file\", \"access_type\", \"timestamp\"],\n",
    "        \"objective\": \"Detect file access events classified as suspicious, such as 'delete' operations.\",\n",
    "        \"example_input\": {\n",
    "            \"user\": [\"WCR0044\", \"LRG0155\"],\n",
    "            \"file\": [\"confidential.pdf\", \"project_plan.docx\"],\n",
    "            \"access_type\": [\"read\", \"delete\"],\n",
    "            \"timestamp\": [\"2024-01-02 05:15:00\", \"2024-01-02 06:40:00\"]\n",
    "        },\n",
    "        \"example_output\": \"Filtered DataFrame with rows flagged for suspicious file access.\"\n",
    "    },\n",
    "    \"email\": {\n",
    "        \"required_columns\": [\"user\", \"email_content\", \"timestamp\"],\n",
    "        \"objective\": \"Detect suspicious email content containing flagged keywords like 'password', 'urgent', or 'secret'.\",\n",
    "        \"example_input\": {\n",
    "            \"user\": [\"WCR0044\", \"LRG0155\"],\n",
    "            \"email_content\": [\"Request for urgent payment\", \"Hello!\"],\n",
    "            \"timestamp\": [\"2024-01-02 05:15:00\", \"2024-01-02 06:40:00\"]\n",
    "        },\n",
    "        \"example_output\": \"Filtered DataFrame containing rows with suspicious email content.\"\n",
    "    },\n",
    "    \"device\": {\n",
    "        \"required_columns\": [\"user\", \"device\", \"action\", \"timestamp\"],\n",
    "        \"objective\": \"Identify unusual device activity, such as connecting unauthorized devices.\",\n",
    "        \"example_input\": {\n",
    "            \"user\": [\"WCR0044\", \"XTR0011\"],\n",
    "            \"device\": [\"USB\", \"Printer\"],\n",
    "            \"action\": [\"connect\", \"disconnect\"],\n",
    "            \"timestamp\": [\"2024-01-02 05:15:00\", \"2024-01-02 06:40:00\"]\n",
    "        },\n",
    "        \"example_output\": \"Filtered DataFrame containing rows with suspicious device activity.\"\n",
    "    },\n",
    "    \"psychometric\": {\n",
    "        \"required_columns\": [\"user\", \"trait\", \"value\"],\n",
    "        \"objective\": \"Analyze psychometric traits to detect scores outside valid ranges or suspicious patterns.\",\n",
    "        \"example_input\": {\n",
    "            \"user\": [\"WCR0044\", \"LRG0155\"],\n",
    "            \"trait\": [\"openness\", \"conscientiousness\"],\n",
    "            \"value\": [0.8, 1.2]\n",
    "        },\n",
    "        \"example_output\": \"Filtered DataFrame highlighting users with psychometric anomalies.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def validate_and_prepare_data(df: pd.DataFrame, required_columns: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Validate and prepare input data for the tool.\n",
    "    Ensures all required columns exist and are in correct formats.\n",
    "    \"\"\"\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            logging.warning(f\"Column '{col}' is missing in input data. Filling with NaN.\")\n",
    "            df[col] = None  # Add missing columns with NaN values\n",
    "\n",
    "    # Ensure datetime columns are properly formatted\n",
    "    for col in required_columns:\n",
    "        if \"date\" in col or \"timestamp\" in col:\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                df[col] = df[col].fillna(pd.Timestamp.now())  # Correct assignment\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error converting column '{col}' to datetime: {e}\")\n",
    "                df[col] = pd.Timestamp.now()  # Default to current time if conversion fails\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_tool_code(agent, task_name: str, requirements: str, log_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate tool code with a detailed prompt including schema, examples, and objectives.\n",
    "    \"\"\"\n",
    "    spec = tool_specs[log_type]\n",
    "    prompt = f\"\"\"\n",
    "    Generate a robust Python function named '{task_name}'.\n",
    "    Objective: {spec['objective']}\n",
    "    Input Schema: The DataFrame must include these columns: {', '.join(spec['required_columns'])}.\n",
    "    Example Input: {spec['example_input']}\n",
    "    Expected Output: {spec['example_output']}\n",
    "    Key Requirements:\n",
    "    - Validate input schema with error handling for missing or invalid columns.\n",
    "    - Handle edge cases, such as empty DataFrames or columns with incorrect datatypes.\n",
    "    - Include detailed comments for each step.\n",
    "    \"\"\"\n",
    "\n",
    "    return agent.generate_tool_code(task_name, requirements, \"default_prompt\", spec[\"required_columns\"], prompt)\n",
    "\n",
    "\n",
    "def execute_pipeline():\n",
    "    subtasks = decomposer_agent.decompose_task()\n",
    "    executor_nodes = {}\n",
    "    results = {}\n",
    "\n",
    "    # Generate tools for each subtask\n",
    "    for log_type, subtask_name in subtasks.items():\n",
    "        logging.info(f\"Creating tool for subtask: {subtask_name}\")\n",
    "        spec = tool_specs.get(log_type, {})\n",
    "        if not spec:\n",
    "            logging.error(f\"No specification found for log type: {log_type}\")\n",
    "            continue\n",
    "\n",
    "        df = data_sources.get(log_type, pd.DataFrame())\n",
    "        df = validate_and_prepare_data(df, spec[\"required_columns\"])\n",
    "\n",
    "        tool_code = generate_tool_code(rl_tool_builder_agent, subtask_name, spec[\"objective\"], log_type)\n",
    "        if tool_code is None:\n",
    "            logging.error(f\"Failed to generate tool for {subtask_name}.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            exec(tool_code, globals())\n",
    "            tool_func = globals().get(subtask_name)\n",
    "            if tool_func:\n",
    "                executor_nodes[log_type] = ExecutorNode(subtask_name, tool_func)\n",
    "            else:\n",
    "                logging.error(f\"Function {subtask_name} not found in globals.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading generated tool for {subtask_name}: {e}\")\n",
    "\n",
    "    # Execute the tools\n",
    "    for log_type, executor_node in executor_nodes.items():\n",
    "        df = data_sources.get(log_type, pd.DataFrame())\n",
    "        results[log_type] = safe_execute_tool(executor_node.tool_func, df) if executor_node.tool_func else None\n",
    "\n",
    "    # Aggregate results\n",
    "    return anomaly_aggregator_agent.aggregate(results)\n",
    "\n",
    "\n",
    "\n",
    "# Run the pipeline\n",
    "final_results = execute_pipeline()\n",
    "\n",
    "# Output the final aggregated anomalies\n",
    "print(\"Final Aggregated Anomalies:\")\n",
    "print(final_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload and validate data_dict\n",
    "data_dict = {\n",
    "    'logon': pd.read_csv(logon_file),\n",
    "    'file_access': pd.read_csv(file_access_file),\n",
    "    'email': pd.read_csv(email_file),\n",
    "    'device': pd.read_csv(device_file),\n",
    "    'http': pd.read_csv(http_file)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using subset size: 100\n",
      "Loading and Subsetting Data: 100%|██████████| 4/4 [00:00<00:00, 33.89it/s]\n",
      "WARNING:root:Column 'content' is missing in input data. Filling with NaN.\n",
      "WARNING:root:Column 'file' is missing in input data. Filling with NaN.\n",
      "WARNING:root:Column 'access_type' is missing in input data. Filling with NaN.\n",
      "WARNING:root:Column 'timestamp' is missing in input data. Filling with NaN.\n",
      "WARNING:root:Column 'email_content' is missing in input data. Filling with NaN.\n",
      "WARNING:root:Column 'timestamp' is missing in input data. Filling with NaN.\n",
      "WARNING:root:Column 'device' is missing in input data. Filling with NaN.\n",
      "WARNING:root:Column 'action' is missing in input data. Filling with NaN.\n",
      "WARNING:root:Column 'timestamp' is missing in input data. Filling with NaN.\n",
      "INFO:root:Decomposed task 'Detect_Suspicious_Activity' into subtasks: ['Detect_Suspicious_Activity_logon', 'Detect_Suspicious_Activity_psychometric', 'Detect_Suspicious_Activity_file_access', 'Detect_Suspicious_Activity_email', 'Detect_Suspicious_Activity_device']\n",
      "INFO:root:Creating tool for subtask: Detect_Suspicious_Activity_logon\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Generated code after 1 attempt(s)\n",
      "INFO:root:Creating tool for subtask: Detect_Suspicious_Activity_psychometric\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Generated code after 1 attempt(s)\n",
      "INFO:root:Creating tool for subtask: Detect_Suspicious_Activity_file_access\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Generated code after 1 attempt(s)\n",
      "INFO:root:Creating tool for subtask: Detect_Suspicious_Activity_email\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Generated code after 1 attempt(s)\n",
      "INFO:root:Creating tool for subtask: Detect_Suspicious_Activity_device\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Generated code after 1 attempt(s)\n",
      "ERROR:root:Unhandled exception during tool execution for Detect_Suspicious_Activity_psychometric: Column 'value' must be a float type\n",
      "ERROR:root:Unhandled exception during tool execution for Detect_Suspicious_Activity_file_access: No suspicious activity detected.\n",
      "ERROR:root:Unhandled exception during tool execution for Detect_Suspicious_Activity_email: Cannot mask with non-boolean array containing NA / NaN values\n",
      "ERROR:root:Unhandled exception during tool execution for Detect_Suspicious_Activity_device: 0\n",
      "INFO:root:Final Aggregated Anomalies:\n",
      "{'logon': {'anomalies':                                            id     user       pc activity  \\\n",
      "date                                                                       \n",
      "2010-02-02 19:00:00  {B2L2-P6OR27JE-6488PNDB}  EKM0667  PC-3339   Logoff   \n",
      "2010-04-07 18:32:00  {C4J0-F6US63RL-9068VERA}  IDM0916  PC-5335   Logoff   \n",
      "2010-10-05 19:10:00  {Y3D0-X6DR91KR-0063ZZIY}  OTH0763  PC-3402   Logoff   \n",
      "2010-01-17 07:59:00  {Z9K1-O8DV87AO-8889VKAP}  DMR0116  PC-2025    Logon   \n",
      "2011-04-07 07:46:00  {J5I5-W4EP35SP-4225IMEC}  ATC0286  PC-0132    Logon   \n",
      "2011-01-06 19:14:00  {P8X0-Y8ID80MH-9818ONHM}  DSC0751  PC-3002   Logoff   \n",
      "2010-03-07 07:56:00  {P1R5-J6AL55RO-9895ZMIX}  ECM0004  PC-3295    Logon   \n",
      "2011-02-12 07:20:00  {C5E9-W6ZJ32HS-7974CRPT}  CMH0589  PC-2101    Logon   \n",
      "2010-12-21 05:58:57  {X8F6-P4NO31WS-7025SBRQ}  WCR0044  PC-6836    Logon   \n",
      "2010-04-30 18:30:00  {W9E8-M5JA13TM-7682CAUJ}  IPB0360  PC-0860   Logoff   \n",
      "2010-01-06 01:16:42  {E6F2-X2CG40VM-0906CHPF}  DFH0356  PC-7031   Logoff   \n",
      "2010-06-15 07:47:00  {E2J8-B1VY27VA-7378KFOT}  AFB0798  PC-1115    Logon   \n",
      "2011-02-11 18:58:00  {O5Z9-H0BJ65SJ-9032IBHE}  THH0741  PC-5369   Logoff   \n",
      "2010-04-20 07:45:00  {N2M0-O1YI18MK-3404KWDC}  WLB0353  PC-3722    Logon   \n",
      "2010-08-19 18:17:00  {T1M3-G0AH07VV-6762XWBY}  EYC0203  PC-0331   Logoff   \n",
      "2010-07-06 07:30:00  {S7P1-F1GQ36HC-3765IXAY}  IBC0320  PC-0274    Logon   \n",
      "2010-05-18 07:36:00  {N4L7-D4ES87KG-1800GUTP}  JSM0860  PC-2573    Logon   \n",
      "2010-09-08 07:30:00  {R7N3-K0QE53XH-4515GQJV}  NKG0573  PC-0549    Logon   \n",
      "2010-06-03 07:29:00  {K1B3-Q5BB70GR-4735GIEW}  SPH0487  PC-5414    Logon   \n",
      "2011-01-03 18:41:07  {X3T4-E7RA58IZ-2443FTEP}  FLC0370  PC-6827   Logoff   \n",
      "2010-10-08 03:06:54  {W8P2-I0BV15FZ-3582KQQL}  JCH0385  PC-5766    Logon   \n",
      "2010-10-18 07:56:00  {L8M2-S4SX95HU-5889WIMX}  YLW0544  PC-9135    Logon   \n",
      "2010-12-22 05:02:18  {S7P4-T6BL55WP-8966XZNP}  NLC0217  PC-3764    Logon   \n",
      "2010-10-29 03:14:04  {W9Z3-Y5WE25GI-9606CJNX}  SDM0375  PC-8366   Logoff   \n",
      "2010-12-01 07:46:00  {J7D1-D2YH07GL-2425GYOZ}  HHB1000  PC-9432    Logon   \n",
      "2010-04-29 01:03:21  {F9Y3-F5OI24BD-5632GRBH}  CSS0046  PC-9548   Logoff   \n",
      "2010-06-24 07:32:00  {H2Y0-Q7EM74DV-8897HLSX}  FPN0308  PC-1705    Logon   \n",
      "2010-06-16 18:58:00  {F8B2-M7LQ78VD-0751GZRD}  ACW0382  PC-1275   Logoff   \n",
      "2010-02-08 06:44:00  {C0S5-X7JI19NM-8291UZWR}  LRG0155  PC-0450    Logon   \n",
      "2010-03-04 18:00:00  {D9J9-E2PL84LY-8555EHIA}  JBW0623  PC-2336   Logoff   \n",
      "2010-06-15 22:35:07  {J0L5-S4TC12KG-3835UTGB}  LBW0372  PC-6359    Logon   \n",
      "2010-02-09 18:00:00  {T4O8-J5XT90UC-1997EHCD}  KLM0524  PC-1889   Logoff   \n",
      "2010-09-02 19:11:00  {R8J6-G2UT77GC-5293JCAO}  OSW0680  PC-0663   Logoff   \n",
      "2010-06-24 07:30:00  {L9B8-E1BN76EX-7780NLSZ}  LIC0092  PC-3824    Logon   \n",
      "2011-03-17 18:07:00  {T2J1-A6RW03DM-4437QZKY}  WTR0529  PC-3464   Logoff   \n",
      "2010-10-11 22:45:27  {Q1L4-T9PT09EE-6523JFXB}  CKH0363  PC-9088   Logoff   \n",
      "2010-03-25 07:15:00  {E2I1-X6GB83GJ-4579RCXF}  BWR0897  PC-4630    Logon   \n",
      "2010-09-10 07:00:00  {K1F4-V4WZ68OI-7729AFXI}  KRP0648  PC-7323    Logon   \n",
      "2010-09-07 18:09:00  {X2T7-C0MO16JE-3592ZDQG}  MCG0130  PC-7542   Logoff   \n",
      "2010-10-28 07:09:00  {Z3O7-U1HF82OL-0696ONSO}  JNL0898  PC-1601    Logon   \n",
      "2011-03-22 18:08:00  {L4L4-D9KV78WU-5277TTRA}  SPN0913  PC-1500   Logoff   \n",
      "2010-12-07 07:59:00  {Y3J9-H3OX32SN-0648GPPU}  IRG0001  PC-4604    Logon   \n",
      "2010-05-05 00:02:30  {C7B7-F0XU20QO-5584KMZV}  LMD0218  PC-6254    Logon   \n",
      "2010-08-26 18:30:00  {U9T2-A0LL70BX-7400ZWCB}  EDS0923  PC-6214   Logoff   \n",
      "2010-11-03 01:16:17  {B4A0-X0MN72CM-6969JPAV}  AAB0224  PC-1614    Logon   \n",
      "2011-01-21 07:00:00  {A7C2-Q6PS76LC-2068TXFE}  KKB0938  PC-6643    Logon   \n",
      "\n",
      "                    content  \n",
      "date                         \n",
      "2010-02-02 19:00:00    None  \n",
      "2010-04-07 18:32:00    None  \n",
      "2010-10-05 19:10:00    None  \n",
      "2010-01-17 07:59:00    None  \n",
      "2011-04-07 07:46:00    None  \n",
      "2011-01-06 19:14:00    None  \n",
      "2010-03-07 07:56:00    None  \n",
      "2011-02-12 07:20:00    None  \n",
      "2010-12-21 05:58:57    None  \n",
      "2010-04-30 18:30:00    None  \n",
      "2010-01-06 01:16:42    None  \n",
      "2010-06-15 07:47:00    None  \n",
      "2011-02-11 18:58:00    None  \n",
      "2010-04-20 07:45:00    None  \n",
      "2010-08-19 18:17:00    None  \n",
      "2010-07-06 07:30:00    None  \n",
      "2010-05-18 07:36:00    None  \n",
      "2010-09-08 07:30:00    None  \n",
      "2010-06-03 07:29:00    None  \n",
      "2011-01-03 18:41:07    None  \n",
      "2010-10-08 03:06:54    None  \n",
      "2010-10-18 07:56:00    None  \n",
      "2010-12-22 05:02:18    None  \n",
      "2010-10-29 03:14:04    None  \n",
      "2010-12-01 07:46:00    None  \n",
      "2010-04-29 01:03:21    None  \n",
      "2010-06-24 07:32:00    None  \n",
      "2010-06-16 18:58:00    None  \n",
      "2010-02-08 06:44:00    None  \n",
      "2010-03-04 18:00:00    None  \n",
      "2010-06-15 22:35:07    None  \n",
      "2010-02-09 18:00:00    None  \n",
      "2010-09-02 19:11:00    None  \n",
      "2010-06-24 07:30:00    None  \n",
      "2011-03-17 18:07:00    None  \n",
      "2010-10-11 22:45:27    None  \n",
      "2010-03-25 07:15:00    None  \n",
      "2010-09-10 07:00:00    None  \n",
      "2010-09-07 18:09:00    None  \n",
      "2010-10-28 07:09:00    None  \n",
      "2011-03-22 18:08:00    None  \n",
      "2010-12-07 07:59:00    None  \n",
      "2010-05-05 00:02:30    None  \n",
      "2010-08-26 18:30:00    None  \n",
      "2010-11-03 01:16:17    None  \n",
      "2011-01-21 07:00:00    None  , 'reasons': 'Flagged 46 suspicious entries for logon'}, 'psychometric': {'anomalies': None, 'reasons': 'No anomalies detected (empty DataFrame) for psychometric'}, 'file_access': {'anomalies': None, 'reasons': 'No anomalies detected (empty DataFrame) for file_access'}, 'email': {'anomalies': None, 'reasons': 'No anomalies detected (empty DataFrame) for email'}, 'device': {'anomalies': None, 'reasons': 'No anomalies detected (empty DataFrame) for device'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Aggregated Anomalies:\n",
      "{'logon': {'anomalies':                                            id     user       pc activity  \\\n",
      "date                                                                       \n",
      "2010-02-02 19:00:00  {B2L2-P6OR27JE-6488PNDB}  EKM0667  PC-3339   Logoff   \n",
      "2010-04-07 18:32:00  {C4J0-F6US63RL-9068VERA}  IDM0916  PC-5335   Logoff   \n",
      "2010-10-05 19:10:00  {Y3D0-X6DR91KR-0063ZZIY}  OTH0763  PC-3402   Logoff   \n",
      "2010-01-17 07:59:00  {Z9K1-O8DV87AO-8889VKAP}  DMR0116  PC-2025    Logon   \n",
      "2011-04-07 07:46:00  {J5I5-W4EP35SP-4225IMEC}  ATC0286  PC-0132    Logon   \n",
      "2011-01-06 19:14:00  {P8X0-Y8ID80MH-9818ONHM}  DSC0751  PC-3002   Logoff   \n",
      "2010-03-07 07:56:00  {P1R5-J6AL55RO-9895ZMIX}  ECM0004  PC-3295    Logon   \n",
      "2011-02-12 07:20:00  {C5E9-W6ZJ32HS-7974CRPT}  CMH0589  PC-2101    Logon   \n",
      "2010-12-21 05:58:57  {X8F6-P4NO31WS-7025SBRQ}  WCR0044  PC-6836    Logon   \n",
      "2010-04-30 18:30:00  {W9E8-M5JA13TM-7682CAUJ}  IPB0360  PC-0860   Logoff   \n",
      "2010-01-06 01:16:42  {E6F2-X2CG40VM-0906CHPF}  DFH0356  PC-7031   Logoff   \n",
      "2010-06-15 07:47:00  {E2J8-B1VY27VA-7378KFOT}  AFB0798  PC-1115    Logon   \n",
      "2011-02-11 18:58:00  {O5Z9-H0BJ65SJ-9032IBHE}  THH0741  PC-5369   Logoff   \n",
      "2010-04-20 07:45:00  {N2M0-O1YI18MK-3404KWDC}  WLB0353  PC-3722    Logon   \n",
      "2010-08-19 18:17:00  {T1M3-G0AH07VV-6762XWBY}  EYC0203  PC-0331   Logoff   \n",
      "2010-07-06 07:30:00  {S7P1-F1GQ36HC-3765IXAY}  IBC0320  PC-0274    Logon   \n",
      "2010-05-18 07:36:00  {N4L7-D4ES87KG-1800GUTP}  JSM0860  PC-2573    Logon   \n",
      "2010-09-08 07:30:00  {R7N3-K0QE53XH-4515GQJV}  NKG0573  PC-0549    Logon   \n",
      "2010-06-03 07:29:00  {K1B3-Q5BB70GR-4735GIEW}  SPH0487  PC-5414    Logon   \n",
      "2011-01-03 18:41:07  {X3T4-E7RA58IZ-2443FTEP}  FLC0370  PC-6827   Logoff   \n",
      "2010-10-08 03:06:54  {W8P2-I0BV15FZ-3582KQQL}  JCH0385  PC-5766    Logon   \n",
      "2010-10-18 07:56:00  {L8M2-S4SX95HU-5889WIMX}  YLW0544  PC-9135    Logon   \n",
      "2010-12-22 05:02:18  {S7P4-T6BL55WP-8966XZNP}  NLC0217  PC-3764    Logon   \n",
      "2010-10-29 03:14:04  {W9Z3-Y5WE25GI-9606CJNX}  SDM0375  PC-8366   Logoff   \n",
      "2010-12-01 07:46:00  {J7D1-D2YH07GL-2425GYOZ}  HHB1000  PC-9432    Logon   \n",
      "2010-04-29 01:03:21  {F9Y3-F5OI24BD-5632GRBH}  CSS0046  PC-9548   Logoff   \n",
      "2010-06-24 07:32:00  {H2Y0-Q7EM74DV-8897HLSX}  FPN0308  PC-1705    Logon   \n",
      "2010-06-16 18:58:00  {F8B2-M7LQ78VD-0751GZRD}  ACW0382  PC-1275   Logoff   \n",
      "2010-02-08 06:44:00  {C0S5-X7JI19NM-8291UZWR}  LRG0155  PC-0450    Logon   \n",
      "2010-03-04 18:00:00  {D9J9-E2PL84LY-8555EHIA}  JBW0623  PC-2336   Logoff   \n",
      "2010-06-15 22:35:07  {J0L5-S4TC12KG-3835UTGB}  LBW0372  PC-6359    Logon   \n",
      "2010-02-09 18:00:00  {T4O8-J5XT90UC-1997EHCD}  KLM0524  PC-1889   Logoff   \n",
      "2010-09-02 19:11:00  {R8J6-G2UT77GC-5293JCAO}  OSW0680  PC-0663   Logoff   \n",
      "2010-06-24 07:30:00  {L9B8-E1BN76EX-7780NLSZ}  LIC0092  PC-3824    Logon   \n",
      "2011-03-17 18:07:00  {T2J1-A6RW03DM-4437QZKY}  WTR0529  PC-3464   Logoff   \n",
      "2010-10-11 22:45:27  {Q1L4-T9PT09EE-6523JFXB}  CKH0363  PC-9088   Logoff   \n",
      "2010-03-25 07:15:00  {E2I1-X6GB83GJ-4579RCXF}  BWR0897  PC-4630    Logon   \n",
      "2010-09-10 07:00:00  {K1F4-V4WZ68OI-7729AFXI}  KRP0648  PC-7323    Logon   \n",
      "2010-09-07 18:09:00  {X2T7-C0MO16JE-3592ZDQG}  MCG0130  PC-7542   Logoff   \n",
      "2010-10-28 07:09:00  {Z3O7-U1HF82OL-0696ONSO}  JNL0898  PC-1601    Logon   \n",
      "2011-03-22 18:08:00  {L4L4-D9KV78WU-5277TTRA}  SPN0913  PC-1500   Logoff   \n",
      "2010-12-07 07:59:00  {Y3J9-H3OX32SN-0648GPPU}  IRG0001  PC-4604    Logon   \n",
      "2010-05-05 00:02:30  {C7B7-F0XU20QO-5584KMZV}  LMD0218  PC-6254    Logon   \n",
      "2010-08-26 18:30:00  {U9T2-A0LL70BX-7400ZWCB}  EDS0923  PC-6214   Logoff   \n",
      "2010-11-03 01:16:17  {B4A0-X0MN72CM-6969JPAV}  AAB0224  PC-1614    Logon   \n",
      "2011-01-21 07:00:00  {A7C2-Q6PS76LC-2068TXFE}  KKB0938  PC-6643    Logon   \n",
      "\n",
      "                    content  \n",
      "date                         \n",
      "2010-02-02 19:00:00    None  \n",
      "2010-04-07 18:32:00    None  \n",
      "2010-10-05 19:10:00    None  \n",
      "2010-01-17 07:59:00    None  \n",
      "2011-04-07 07:46:00    None  \n",
      "2011-01-06 19:14:00    None  \n",
      "2010-03-07 07:56:00    None  \n",
      "2011-02-12 07:20:00    None  \n",
      "2010-12-21 05:58:57    None  \n",
      "2010-04-30 18:30:00    None  \n",
      "2010-01-06 01:16:42    None  \n",
      "2010-06-15 07:47:00    None  \n",
      "2011-02-11 18:58:00    None  \n",
      "2010-04-20 07:45:00    None  \n",
      "2010-08-19 18:17:00    None  \n",
      "2010-07-06 07:30:00    None  \n",
      "2010-05-18 07:36:00    None  \n",
      "2010-09-08 07:30:00    None  \n",
      "2010-06-03 07:29:00    None  \n",
      "2011-01-03 18:41:07    None  \n",
      "2010-10-08 03:06:54    None  \n",
      "2010-10-18 07:56:00    None  \n",
      "2010-12-22 05:02:18    None  \n",
      "2010-10-29 03:14:04    None  \n",
      "2010-12-01 07:46:00    None  \n",
      "2010-04-29 01:03:21    None  \n",
      "2010-06-24 07:32:00    None  \n",
      "2010-06-16 18:58:00    None  \n",
      "2010-02-08 06:44:00    None  \n",
      "2010-03-04 18:00:00    None  \n",
      "2010-06-15 22:35:07    None  \n",
      "2010-02-09 18:00:00    None  \n",
      "2010-09-02 19:11:00    None  \n",
      "2010-06-24 07:30:00    None  \n",
      "2011-03-17 18:07:00    None  \n",
      "2010-10-11 22:45:27    None  \n",
      "2010-03-25 07:15:00    None  \n",
      "2010-09-10 07:00:00    None  \n",
      "2010-09-07 18:09:00    None  \n",
      "2010-10-28 07:09:00    None  \n",
      "2011-03-22 18:08:00    None  \n",
      "2010-12-07 07:59:00    None  \n",
      "2010-05-05 00:02:30    None  \n",
      "2010-08-26 18:30:00    None  \n",
      "2010-11-03 01:16:17    None  \n",
      "2011-01-21 07:00:00    None  , 'reasons': 'Flagged 46 suspicious entries for logon'}, 'psychometric': {'anomalies': None, 'reasons': 'No anomalies detected (empty DataFrame) for psychometric'}, 'file_access': {'anomalies': None, 'reasons': 'No anomalies detected (empty DataFrame) for file_access'}, 'email': {'anomalies': None, 'reasons': 'No anomalies detected (empty DataFrame) for email'}, 'device': {'anomalies': None, 'reasons': 'No anomalies detected (empty DataFrame) for device'}}\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "def load_and_subset_data(data_dict, subset_size=None):\n",
    "    \"\"\"\n",
    "    Load and optionally subset data from the provided data dictionary.\n",
    "    \n",
    "    Args:\n",
    "        data_dict (dict): Dictionary containing DataFrames for different log types.\n",
    "        subset_size (int): Number of rows to select for each DataFrame. If None, uses the entire dataset.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing DataFrames (subset if specified).\n",
    "    \"\"\"\n",
    "    subset_data = {}\n",
    "    for log_type, df in tqdm(data_dict.items(), desc=\"Loading and Subsetting Data\"):\n",
    "        if subset_size:\n",
    "            # Ensure reproducibility for subsets\n",
    "            random.seed(42)\n",
    "            subset_data[log_type] = df.sample(n=min(subset_size, len(df)), random_state=42)\n",
    "        else:\n",
    "            subset_data[log_type] = df\n",
    "    return subset_data\n",
    "\n",
    "\n",
    "def execute_pipeline_with_subset(data_dict, subset_size=None):\n",
    "    \"\"\"\n",
    "    Execute the anomaly detection pipeline with an optional subset size.\n",
    "\n",
    "    Args:\n",
    "        data_dict (dict): Dictionary containing DataFrames for different log types.\n",
    "        subset_size (int): Number of rows to use for each dataset. If None, uses the full dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: Final aggregated anomalies.\n",
    "    \"\"\"\n",
    "    # Filter only the log types present in tool_specs\n",
    "    filtered_data_dict = {k: v for k, v in data_dict.items() if k in tool_specs}\n",
    "    \n",
    "    # Load and subset data\n",
    "    logging.info(f\"Using subset size: {subset_size}\")\n",
    "    subset_data = load_and_subset_data(filtered_data_dict, subset_size)\n",
    "\n",
    "    # Update data_sources with the subset data\n",
    "    for log_type, df in subset_data.items():\n",
    "        data_sources[log_type] = validate_and_prepare_data(df, tool_specs[log_type][\"required_columns\"])\n",
    "\n",
    "    # Execute pipeline\n",
    "    return execute_pipeline()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "subset_size = 100  # Adjust subset size as needed\n",
    "final_results = execute_pipeline_with_subset(data_dict, subset_size=subset_size)\n",
    "\n",
    "# Output the results\n",
    "print(\"Final Aggregated Anomalies:\")\n",
    "print(final_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user': ['WCR0044', 'LRG0155'],\n",
       " 'trait': ['openness', 'conscientiousness'],\n",
       " 'value': [0.8, 1.2]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt-hacks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
